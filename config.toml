[server]
host = "0.0.0.0"
port = 8000

# VLLM with Qwen 2.5 72B 8bit - 15K Context
[vllm-qwen25-72b]
host = "10.101.100.11"
port = 8021
max_tokens = 2_000
temperature = 0.1

# VLLM with Deepseek R1 32B 8bit - 20K Context
[vllm-deepseek-r1-32b]
host = "10.101.100.11"
port = 8025
max_tokens = 2_000
temperature = 0.1

[app]
correction = { token_limit = 3_000 }
summary = { token_limit = 3_000 }
